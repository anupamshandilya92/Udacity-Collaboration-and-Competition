{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 : Collaboration and Competition\n",
    "\n",
    "## Project's goal\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  **Thus, the goal of each agent is to keep the ball in play.**\n",
    "\n",
    "The task is episodic, and in order to solve the environment, **the agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents)**. Specifically,\n",
    "\n",
    "- After each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores.\n",
    "- This yields a single **score** for each episode.\n",
    "\n",
    "The environment is considered solved, when the average (over 100 episodes) of those **scores is at least +0.5.**\n",
    "\n",
    "\n",
    "## Environment details\n",
    "\n",
    "The environment is based on [Unity ML-agents](https://github.com/Unity-Technologies/ml-agents). The project environment provided by Udacity is similar to the [Tennis](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#tennis) environment on the Unity ML-Agents GitHub page.\n",
    "\n",
    "> The Unity Machine Learning Agents Toolkit (ML-Agents) is an open-source Unity plugin that enables games and simulations to serve as environments for training intelligent agents. Agents can be trained using reinforcement learning, imitation learning, neuroevolution, or other machine learning methods through a simple-to-use Python API. \n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation.  Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "- Set-up: Two-player game where agents control rackets to bounce ball over a net.\n",
    "- Goal: The agents must bounce ball between one another while not dropping or sending ball out of bounds.\n",
    "- Agents: The environment contains two agent linked to a single Brain named TennisBrain. After training you can attach another Brain named MyBrain to one of the agent to play against your trained model.\n",
    "- Agent Reward Function (independent):\n",
    "  - +0.1 To agent when hitting ball over net.\n",
    "  - -0.1 To agent who let ball hit their ground, or hit ball out of bounds.\n",
    "- Brains: One Brain with the following observation/action space.\n",
    "- Vector Observation space: 8 variables corresponding to position and velocity of ball and racket.\n",
    "  - In the Udacity provided environment, 3 observations are stacked (8 *3 = 24 variables) \n",
    "- Vector Action space: (Continuous) Size of 2, corresponding to movement toward net or away from net, and jumping.\n",
    "- Visual Observations: None.\n",
    "- Reset Parameters: One, corresponding to size of ball.\n",
    "- Benchmark Mean Reward: 2.5\n",
    "- Optional Imitation Learning scene: TennisIL.\n",
    "\n",
    "\n",
    "\n",
    "## Agent Implementation\n",
    "\n",
    "This project uses an *off-policy method* called **Multi Agent Deep Deterministic Policy Gradient (MADDPG)** algorithm.\n",
    "\n",
    "### Background for Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "MADDPG find its origins in an *off-policy method* called **Deep Deterministic Policy Gradient (DDPG)** and described in the paper [Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971). \n",
    "\n",
    "> We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.\n",
    "\n",
    "Deep Deterministic Policy Gradient (DDPG) is an algorithm which concurrently learns a Q-function and a policy. It uses off-policy data and the Bellman equation to learn the Q-function, and uses the Q-function to learn the policy.\n",
    "\n",
    "More details available on the Open AI's [Spinning Up](https://spinningup.openai.com/en/latest/algorithms/ddpg.html) website.\n",
    "\n",
    "![DDPG algorithm from Spinning Up website]\n",
    "\n",
    "This algorithm screenshot is taken from the [DDPG algorithm from the Spinning Up website](https://spinningup.openai.com/en/latest/algorithms/ddpg.html)\n",
    "\n",
    "### Multi Agent Deep Deterministic Policy Gradient (MADDPG)\n",
    "\n",
    "For this project I have used a variant of DDPG called **Multi Agent Deep Deterministic Policy Gradient (MADDPG)** which is  described in the paper [Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/abs/1706.02275)\n",
    "\n",
    "> We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.\n",
    "\n",
    "> we accomplish our goal by adopting the framework of centralized training with\n",
    "decentralized execution. Thus, we allow the policies to use extra information to ease training, so\n",
    "long as this information is not used at test time. It is unnatural to do this with Q-learning, as the Q\n",
    "function generally cannot contain different information at training and test time. Thus, we propose\n",
    "a simple extension of actor-critic policy gradient methods where the critic is augmented with extra\n",
    "information about the policies of other agents.\n",
    "\n",
    "In short, this means that during the training, the Critics networks have access to the states and actions information of both agents, while the Actors networks have only access to the information corresponding to their local agent.\n",
    "\n",
    "### Code implementation\n",
    "\n",
    "The code used here is derived from the \"DDPG pidedal\" tutorial from the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893), and modified to implement the **Multi-Agent Actor Critic**  as discussed in the Multi-Agent Udacity tutorial lesson.\n",
    "\n",
    "The code is written in [Python 3.6](https://www.python.org/downloads/release/python-360/) and is relying on [PyTorch 0.4.0](https://pytorch.org/docs/0.4.0/) framework.\n",
    "\n",
    "The code consist of :\n",
    "\n",
    "- `model.py` : Implement the **Actor** and the **Critic** classes.\n",
    "    - The Actor and Critic classes each implement a *Target* and a *Local* Neural Networks used for the training.\n",
    "\n",
    "- `maddpg_agent.py`: Implement the MADDPG alorithm. \n",
    "  - The `maddpg` is relying on the `ddpg` class\n",
    "   - It instanciates DDPG Agents\n",
    "   - It provides a helper function to save the models checkpoints\n",
    "   - It provides the `step()` and `act()` methods\n",
    "   - As the **Multi-Agent Actor Critic** `learn()` function slightly differs from the DDPG one, a `maddpg_learn()` method is provided here.\n",
    "    - The `learn()` method updates the policy and value parameters using given batch of experience tuples.\n",
    "        ```\n",
    "        Q_targets = r + Î³ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(states) -> action\n",
    "            critic_target(all_states, all_actions) -> Q-value\n",
    "        ```  \n",
    "    \n",
    "- `ddpg_agent.py` : Implement the **DDPG agent** and a **Replay Buffer memory** used by the DDPG agent.\n",
    "    - The Actor's *Local* and *Target* neural networks, and the Critic's *Local* and *Target* neural networks are instanciated by the Agent's constructor\n",
    "    - The `learn()` method is specific to DDPG and is not used in this project (I keep it for code later code reuse)\n",
    "  \n",
    "- `memory.py` : Implement the Buffer Replay Memory\n",
    "    - As it is accessed by both Agents, it is instanciated in the maddpg class instead of the ddpg class.\n",
    "    \n",
    "- `utils.py` : Implement some helper functions to encode the states and actions before being inserted in the Replay Buffer, and decode them when a batch of experience is sampled from the Replay Buffer (I wanted the Memory Buffer code to remain unmodified, and thus be usable 'as this' for Single Agent or Multiple Agents)\n",
    "\n",
    "- `hyperparameters.py` : Defines all the hyperparameters in constant variables. (**Important**: Don't forget to restart the Jupyter Notebook Kernel to take into account any change done to these parameters)\n",
    "\n",
    "- `TennisProject.ipynb` : This Jupyter notebooks allows to instanciate and train both agent. More in details it allows to :\n",
    "  - Prepare the Unity environment and Import the necessary packages \n",
    "  - Check the Unity environment\n",
    "  - Define a helper function to instanciate and train a MADDPG agent\n",
    "  - Train an agent using MADDPG \n",
    "  - Plot the score results\n",
    "\n",
    "### MADDPG parameters and results\n",
    "\n",
    "#### Methodology\n",
    "\n",
    "\n",
    "- Results : \"Environment solved in 2293 episodes with an Average Score of 0.51\"\n",
    "\n",
    "\n",
    "\n",
    "#### MADDPG parameters\n",
    "\n",
    "These parameters are in the `hyperparameters.py`  file. They are:\n",
    "\n",
    "```\n",
    "SEED = 10                          # Random seed\n",
    "\n",
    "NB_EPISODES = 10000                # Max nb of episodes\n",
    "NB_STEPS = 1000                    # Max nb of steps per episodes \n",
    "UPDATE_EVERY_NB_EPISODE = 4        # Nb of episodes between learning process\n",
    "MULTIPLE_LEARN_PER_UPDATE = 3      # Nb of multiple learning process performed in a row\n",
    "\n",
    "BUFFER_SIZE = int(1e5)             # replay buffer size\n",
    "BATCH_SIZE = 200                   # minibatch size\n",
    "\n",
    "ACTOR_FC1_UNITS = 400              # Number of units for the layer 1 in the actor model\n",
    "ACTOR_FC2_UNITS = 300              # Number of units for the layer 2 in the actor model\n",
    "CRITIC_FCS1_UNITS = 400            # Number of units for the layer 1 in the critic model\n",
    "CRITIC_FC2_UNITS = 300             # Number of units for the layer 2 in the critic model\n",
    "NON_LIN = F.relu                   # Non linearity operator used in the model\n",
    "LR_ACTOR = 1e-4                    # Learning rate of the actor \n",
    "LR_CRITIC = 5e-3   #2e-3           # Learning rate of the critic\n",
    "WEIGHT_DECAY = 0                   # L2 weight decay\n",
    "\n",
    "GAMMA = 0.995 #0.99                # Discount factor\n",
    "TAU = 1e-3                         # For soft update of target parameters\n",
    "CLIP_CRITIC_GRADIENT = False       # Clip gradient during Critic optimization\n",
    "\n",
    "ADD_OU_NOISE = True                # Add Ornstein-Uhlenbeck noise\n",
    "MU = 0.                            # Ornstein-Uhlenbeck noise parameter\n",
    "THETA = 0.15                       # Ornstein-Uhlenbeck noise parameter\n",
    "SIGMA = 0.2                        # Ornstein-Uhlenbeck noise parameter\n",
    "NOISE = 1.0                        # Initial Noise Amplitude \n",
    "NOISE_REDUCTION = 1.0              # Noise amplitude decay ratio\n",
    "```\n",
    "\n",
    "The **Actor Neural Networks** use the following architecture :\n",
    "\n",
    "```\n",
    "Input nodes (8x3=24 states ) \n",
    "  -> Fully Connected Layer (400 units, Relu activation) \n",
    "    -> Batch Normlization\n",
    "      -> Fully Connected Layer (300 units, Relu activation) \n",
    "         -> Ouput nodes (2 units/actions, tanh activation)\n",
    "```\n",
    "\n",
    "\n",
    "The **Critic Neural Networks** use the following architecture :\n",
    "\n",
    "```\n",
    "Input nodes ( [ 8x3=24 states + 2 actions ] x 2 Agents = 52  ) \n",
    "  -> Fully Connected Layer (400 units, Relu activation) \n",
    "    -> Batch Normlization\n",
    "      -> Fully Connected Layer (300 units, Relu activation) \n",
    "        -> Ouput node (1 unit, no activation)\n",
    "```\n",
    "            \n",
    "\n",
    "Both Neural Networks use the Adam optimizer with a learning rate of 1e-4 (actors) and 5e-3 (critics) are trained using a batch size of 200.\n",
    "\n",
    "\n",
    "\n",
    "### Ideas for future work\n",
    "\n",
    "I would like to implement below models for future work:\n",
    "1.Twin Delayed DDPG (TD3) (https://spinningup.openai.com/en/latest/algorithms/td3.html)\n",
    "2.[Prioritized experience replay](https://arxiv.org/abs/1511.05952)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
